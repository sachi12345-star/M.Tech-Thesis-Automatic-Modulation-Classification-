{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGAmA4zMq2Ax"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4DufZhFupcG"
      },
      "outputs": [],
      "source": [
        "#importing lib\n",
        "import os\n",
        "import seaborn as sb\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNrQ9NTW0gux"
      },
      "outputs": [],
      "source": [
        "path=\"/content/drive/MyDrive/16.0/training\"\n",
        "train=image_dataset_from_directory(path,batch_size=32,image_size=(224, 224), shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edempYFx0hqO"
      },
      "outputs": [],
      "source": [
        "path=\"/content/drive/MyDrive/16.0/validation\"\n",
        "valid=image_dataset_from_directory(path,batch_size=32,image_size=(224, 224), shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBLDxHPl0it1"
      },
      "outputs": [],
      "source": [
        "path=\"/content/drive/MyDrive/16.0/testing\"\n",
        "test=image_dataset_from_directory(path,batch_size=32,image_size=(224, 224), shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmAQ9y37iDEU"
      },
      "source": [
        "Create a list to store spectograms and labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dysQdFdUhG7S"
      },
      "outputs": [],
      "source": [
        "spectrograms = []\n",
        "labels = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc52uSOhiXxu"
      },
      "source": [
        "Label encoding process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nI-0W6SFmA0E"
      },
      "outputs": [],
      "source": [
        "spectrogram_folder= \"/content/drive/MyDrive/16.0_noisy/testing\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiZtCj7qmgKL"
      },
      "outputs": [],
      "source": [
        "input_shape = (224,224, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNdSnPbYk2Ze"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQ2wdtC0L5Sr"
      },
      "outputs": [],
      "source": [
        "for file_name in tqdm(os.listdir(spectrogram_folder)):\n",
        "        temp=os.path.join(spectrogram_folder,file_name)\n",
        "        for i in os.listdir(temp):\n",
        "          # Load the spectrogram image and convert it to a numpy array\n",
        "\n",
        "          img_path = os.path.join(spectrogram_folder, file_name,i)\n",
        "          img = load_img(img_path, target_size=(224,224))\n",
        "          img_array = img_to_array(img)\n",
        "\n",
        "          # Append the spectrogram image and its corresponding label to the lists\n",
        "          spectrograms.append(img_array)\n",
        "          label =file_name\n",
        "          labels.append(label)\n",
        "          # print(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOX0bZ-7jdBx"
      },
      "outputs": [],
      "source": [
        "spectrograms = np.array(spectrograms)\n",
        "labels = np.array(labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "nH90Qeuka-Tv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7d5wxBtjihg"
      },
      "outputs": [],
      "source": [
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(labels)\n",
        "df = pd.DataFrame({'Original Label': labels, 'Encoded Label': encoded_labels})\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
        "\n",
        "# Print mapping\n",
        "for original, encoded in label_mapping.items():\n",
        "    print(f\"'{original}' â†’ {encoded}\")"
      ],
      "metadata": {
        "id": "3w6Rc-DSbVL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0RmOocEjjyy"
      },
      "outputs": [],
      "source": [
        "# Convert the labels to one-hot encoding\n",
        "labels = to_categorical(label_encoder.transform(labels)) # Change to this line\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Agcwk1LuONel"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_spectrograms, test_spectrograms, train_labels, test_labels = train_test_split(spectrograms, labels, test_size=0.2, stratify=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHuuPA-A07M4"
      },
      "outputs": [],
      "source": [
        "train_spectrograms.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wAtYAWmYmyJ"
      },
      "source": [
        "Improved-InceptionV3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VubIK05xYlpZ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Parameters\n",
        "\n",
        "num_classes = 10  # Change this to the number of classes in your dataset\n",
        "learning_rate = 0.0001\n",
        "\n",
        "\n",
        "# Load the Xception model pre-trained on ImageNet\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "\n",
        "# Add custom layers on top of the base model\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "# Combine the base model with the custom layers\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Freeze the layers of the base model to retain pre-trained weights\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(train, validation_data=valid, epochs=10, batch_size=32)\n",
        "# # Print model summary\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#  fine-tune the entire model by unfreezing the base model layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Re-compile the model with a lower learning rate\n",
        "model.compile(optimizer=Adam(learning_rate=learning_rate / 10),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "history = model.fit(train, validation_data=valid, epochs=20, batch_size=32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huTJXl6JT_RH"
      },
      "source": [
        "Improved-Xception"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ts7m4k6lT2ar"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import Xception\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Parameters\n",
        "\n",
        "num_classes = 10  # Change this to the number of classes in your dataset\n",
        "learning_rate = 0.0001\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "# Load the Xception model pre-trained on ImageNet\n",
        "base_model = Xception(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "\n",
        "# Add custom layers on top of the base model\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "# Combine the base model with the custom layers\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Freeze the layers of the base model to retain pre-trained weights\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(train, validation_data=valid, epochs=10, batch_size=32)\n",
        "# Print model summary\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#  fine-tune the entire model by unfreezing the base model layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Re-compile the model with a lower learning rate\n",
        "model.compile(optimizer=Adam(learning_rate=learning_rate / 10),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "history = model.fit(train, validation_data=valid, epochs=20, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdIaS1TYUWI-"
      },
      "source": [
        "Improved-ResNet152V2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75eBBvjTUCTz"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet152V2\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Parameters\n",
        "\n",
        "num_classes = 10  # Change this to the number of classes in your dataset\n",
        "learning_rate = 0.0001\n",
        "\n",
        "\n",
        "# Load the Xception model pre-trained on ImageNet\n",
        "base_model = ResNet152V2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "\n",
        "# Add custom layers on top of the base model\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "# Combine the base model with the custom layers\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Freeze the layers of the base model to retain pre-trained weights\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(train, validation_data=valid, epochs=10, batch_size=32)\n",
        "# # Print model summary\n",
        "# model.summary()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#  fine-tune the entire model by unfreezing the base model layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Re-compile the model with a lower learning rate\n",
        "model.compile(optimizer=Adam(learning_rate=learning_rate / 10),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "history=model.fit(train, validation_data=valid, epochs=20, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time"
      ],
      "metadata": {
        "id": "ZYkn3_om5nWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "\n",
        "# Sample input from validation set\n",
        "sample_input = next(iter(valid))[0][:1]  # Take 1 image from the validation set\n",
        "\n",
        "# Warm-up (to avoid first-run overhead)\n",
        "_ = model.predict(sample_input)\n",
        "\n",
        "# Measure inference time\n",
        "start_time = time.time()\n",
        "_ = model.predict(sample_input)\n",
        "end_time = time.time()\n",
        "\n",
        "inference_time = end_time - start_time\n",
        "print(f\"Inference time per image: {inference_time:.4f} seconds\")\n"
      ],
      "metadata": {
        "id": "gtL4yfOG5luu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"model_3.h5\")\n",
        "\n",
        "import os\n",
        "model_size = os.path.getsize(\"model_3.h5\") / (1024 ** 2)  # Convert to MB\n",
        "print(f\"Model size: {model_size:.2f} MB\")\n"
      ],
      "metadata": {
        "id": "YlIDiP_S5pU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainable_params = np.sum([np.prod(v.get_shape()) for v in model.trainable_weights])\n",
        "non_trainable_params = np.sum([np.prod(v.get_shape()) for v in model.non_trainable_weights])\n",
        "total_params = trainable_params + non_trainable_params\n",
        "\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"Non-trainable parameters: {non_trainable_params:,}\")\n",
        "print(f\"Total parameters: {total_params:,}\")\n"
      ],
      "metadata": {
        "id": "D__iuEq8515X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raQl3FOWOnE5"
      },
      "outputs": [],
      "source": [
        "# # Compile the model\n",
        "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# # Train the model on the training data\n",
        "# train_history = model.fit(train_spectrograms, train_labels, epochs=40, batch_size=8, validation_split=0.3)\n",
        "\n",
        "# # Evaluate the model on the testing data\n",
        "# test_loss, test_accuracy = model.evaluate(test_spectrograms, test_labels)\n",
        "\n",
        "# # Optionally, you can also access the training history\n",
        "train_loss = history.history['loss']\n",
        "train_accuracy = history.history['accuracy']\n",
        "val_loss = history.history['val_loss']\n",
        "val_accuracy = history.history['val_accuracy']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsNbHFMLERAk"
      },
      "outputs": [],
      "source": [
        "# # Plot training and validation loss\n",
        "# plt.plot(train_loss, label='Training Loss')\n",
        "# plt.plot(val_loss, label='Validation Loss')\n",
        "# plt.xlabel('Epochs')\n",
        "# plt.ylabel('Loss')\n",
        "# plt.title(\"SNR 12\")\n",
        "# plt.legend()\n",
        "# plt.show()\n",
        "\n",
        "# Plot training and validation accuracy\n",
        "plt.plot(train_accuracy, label='Training Accuracy')\n",
        "plt.plot(val_accuracy, label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title(\"SNR 12\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXNsakVeh-Xh"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Generate predictions for the test set\n",
        "y_pred = model.predict(test_spectrograms)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "conf_matrix = confusion_matrix(np.argmax(test_labels, axis=1), y_pred_classes)\n",
        "\n",
        "# Define the class labels\n",
        "class_labels = label_encoder.classes_\n",
        "\n",
        "# Print the confusion matrix with class labels\n",
        "print(\"Confusion matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"\")\n",
        "\n",
        "# Generate and print the classification report\n",
        "class_report = classification_report(np.argmax(test_labels, axis=1), y_pred_classes, target_names=class_labels)\n",
        "print(\"Classification report:\")\n",
        "print(class_report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nx2ck7HKX-UV"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Greens\", xticklabels=class_labels, yticklabels=class_labels)\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel(\"Predicted labels\")\n",
        "plt.ylabel(\"True labels\")\n",
        "plt.title(\"Confusion Matrix for SNR 12.0\")\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZXxZPaEhiNm"
      },
      "source": [
        "Visulaizing CNN Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wmf-GF3K8UVV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
        "    grad_model = tf.keras.models.Model(\n",
        "        [model.inputs],\n",
        "        [model.get_layer(last_conv_layer_name).output, model.output]\n",
        "    )\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        conv_outputs, predictions = grad_model(img_array)\n",
        "        if pred_index is None:\n",
        "            pred_index = tf.argmax(predictions[0])\n",
        "        class_channel = predictions[:, pred_index]\n",
        "\n",
        "    grads = tape.gradient(class_channel, conv_outputs)\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "    conv_outputs = conv_outputs[0]\n",
        "\n",
        "    heatmap = tf.reduce_sum(tf.multiply(pooled_grads, conv_outputs), axis=-1)\n",
        "    heatmap = np.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
        "    return heatmap.numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def superimpose_heatmap(img, heatmap, alpha=0.4):\n",
        "    heatmap_resized = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
        "    heatmap_colored = cv2.applyColorMap(np.uint8(255 * heatmap_resized), cv2.COLORMAP_JET)\n",
        "    superimposed_img = heatmap_colored * alpha + img\n",
        "    return np.uint8(superimposed_img)\n"
      ],
      "metadata": {
        "id": "dGBZt2z4Yolz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Noise image testing"
      ],
      "metadata": {
        "id": "h81pZokJeN3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "# -------- Model and Configuration --------\n",
        "img_path = '/content/drive/MyDrive/16.0/testing/OFDM_QAM64/replica_5.png'\n",
        "input_size = (224, 224)\n",
        "last_conv_layer_name = 'post_relu'  # Use the last conv layer in your model\n",
        "\n",
        "# -------- Add Colored Gaussian Noise --------\n",
        "def add_colored_gaussian_noise(image_path, mean=0, std=25):\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.resize(image, input_size)\n",
        "    image = image.astype(np.float32)\n",
        "    noise = np.random.normal(loc=mean, scale=std, size=image.shape).astype(np.float32)\n",
        "    noisy_image = np.clip(image + noise, 0, 255).astype(np.uint8)\n",
        "    return noisy_image\n",
        "\n",
        "# -------- Grad-CAM function --------\n",
        "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
        "    grad_model = tf.keras.models.Model(\n",
        "        inputs=[model.inputs],\n",
        "        outputs=[model.get_layer(last_conv_layer_name).output, model.output]\n",
        "    )\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        conv_outputs, predictions = grad_model(img_array)\n",
        "        if pred_index is None:\n",
        "            pred_index = tf.argmax(predictions[0])\n",
        "        class_channel = predictions[:, pred_index]\n",
        "\n",
        "    grads = tape.gradient(class_channel, conv_outputs)\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "    conv_outputs = conv_outputs[0]\n",
        "    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n",
        "    heatmap = tf.squeeze(heatmap)\n",
        "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
        "    return heatmap.numpy()\n",
        "\n",
        "# -------- Superimpose heatmap --------\n",
        "def superimpose_heatmap(original_img, heatmap, alpha=0.4, colormap=cv2.COLORMAP_JET):\n",
        "    heatmap = cv2.resize(heatmap, (original_img.shape[1], original_img.shape[0]))\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "    heatmap_colored = cv2.applyColorMap(heatmap, colormap)\n",
        "    original_bgr = cv2.cvtColor(original_img, cv2.COLOR_RGB2BGR)\n",
        "    superimposed_img = cv2.addWeighted(heatmap_colored, alpha, original_bgr, 1 - alpha, 0)\n",
        "    return superimposed_img\n",
        "\n",
        "# -------- Generate noisy image and preprocess --------\n",
        "noisy_img = add_colored_gaussian_noise(img_path)\n",
        "noisy_img_rgb = cv2.cvtColor(noisy_img, cv2.COLOR_BGR2RGB)\n",
        "input_noisy = np.expand_dims(noisy_img_rgb, axis=0) / 255.0  # Normalize\n",
        "\n",
        "# -------- Predict and Grad-CAM --------\n",
        "pred_probs = model.predict(input_noisy)[0]\n",
        "predicted_class = np.argmax(pred_probs)\n",
        "\n",
        "heatmap_noisy = make_gradcam_heatmap(input_noisy, model, last_conv_layer_name, pred_index=predicted_class)\n",
        "cam_image_noisy = superimpose_heatmap(noisy_img_rgb, heatmap_noisy)\n",
        "\n",
        "# -------- Display Results --------\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "original = cv2.imread(img_path)\n",
        "original = cv2.resize(original, input_size)\n",
        "plt.title(\"Original Image\")\n",
        "plt.imshow(cv2.cvtColor(original, cv2.COLOR_BGR2RGB))\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.title(\"Colored Noisy Image\")\n",
        "plt.imshow(noisy_img_rgb)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.title(\"Grad-CAM (Noisy Image)\")\n",
        "plt.imshow(cv2.cvtColor(cam_image_noisy, cv2.COLOR_BGR2RGB))\n",
        "plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lYK7WOT0eJQU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}